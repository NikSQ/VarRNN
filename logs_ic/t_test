from copy import deepcopy
import os

import sys
#sys.path.append("/clusterFS/home/student/kopp13/VarRNN/")
sys.path.append("/home/niki/workspace/VarRNN")

import numpy as np
import tensorflow as tf

from src.configuration.data_config import DataConfig
from src.configuration.nn_config import NNConfig, LSTMLayerConfig, FFLayerConfig, InputLayerConfig
from src.configuration.weight_config import WeightConfig, GaussianWeightConfig, DiscreteWeightConfig
from src.configuration.train_config import TrainConfig, LayerTrainConfig
from src.configuration.info_config import InfoConfig
from src.configuration.constants import WeightC, NetworkC, AlgorithmC

from src.network_gpu.experiment import Experiment
os.environ['CUDA_VISIBLE_DEVICES'] = ""
from src.data.loader import load_api_datasets

write_logs_to_file = True
def run_t(task_id):
    if write_logs_to_file:
        orig_stdout = sys.stdout
        f = open('../logs_ic/t_exp_' + str(task_id) + '.txt', 'w')
        sys.stdout = f

    epochs = 600
    runs = 1
    exp_id = int(task_id / 3)
    run_id = task_id % 3

    #
    #
    # =====================================================================================================================
    # DATASET CONFIGURATION
    # =====================================================================================================================
    #
    #

    data_config = DataConfig()
    data_config.add_timit()

    #
    #
    # =====================================================================================================================
    # MODEL CONFIGURATION
    # =====================================================================================================================
    #
    #

    nn_config = NNConfig()
    priors = [1., 3., 1.]
    weight_type = [WeightC.TERNARY, WeightC.BINARY, WeightC.TERNARY][exp_id % 3]

    # Specification of weight properties which are then used in layers
    b_config = GaussianWeightConfig().set_initializers(mean_initializer=(WeightC.XAVIER_INIT, None), logvar_initializer=(WeightC.CONSTANT_INIT, np.log(0.001)))
    bi_config = GaussianWeightConfig().set_initializers(mean_initializer=(WeightC.CONSTANT_INIT, -1.5), logvar_initializer=(WeightC.CONSTANT_INIT, np.log(0.001)))
    #w_config = DiscreteWeightConfig(dist=WeightC.TERNARY, parametrization=WeightC.SIGMOID)
    w_config = DiscreteWeightConfig(dist=weight_type, parametrization=WeightC.LOGIT)#.set_priors(priors)
    #w_config = GaussianWeightConfig().set_initializers(mean_initializer=WeightC.XAVIER_INIT)



    discrete_gates = [[], [], [NetworkC.CANDIDATE_GATE, NetworkC.OUTPUT_GATE]][exp_id % 3]

    continuous_gates = [[NetworkC.INPUT_GATE, NetworkC.CANDIDATE_GATE, NetworkC.OUTPUT_GATE],
                        [NetworkC.INPUT_GATE, NetworkC.CANDIDATE_GATE, NetworkC.OUTPUT_GATE],
                       [NetworkC.INPUT_GATE]][exp_id % 3]
    
    filename = ["t_model_ternary", "t_model_binary", "t_model_dacts"][exp_id % 3]



    # For each layer, properties of weight and configuration function is set
    lstm_layer_config = LSTMLayerConfig()\
        .set_gates_config(gates=[NetworkC.INPUT_GATE],
                        weight_config=w_config, bias_config=bi_config)\
        .set_gates_config(gates=[NetworkC.OUTPUT_GATE, NetworkC.CANDIDATE_GATE],
                        weight_config=w_config, bias_config=b_config)\
        .set_act_funcs_codomain(discrete_gates=discrete_gates,
                                continuous_gates=continuous_gates)
    ff_layer_config = FFLayerConfig()\
        .set_weight_config(weight_config=w_config, bias_config=b_config)


    nn_config.add_layer_config(n_neurons=13, layer_config=InputLayerConfig())
    nn_config.add_layer_config(n_neurons=70, layer_config=lstm_layer_config, var_scope="lstm_0")
    nn_config.add_layer_config(n_neurons=100, layer_config=lstm_layer_config, var_scope="lstm_1")
    nn_config.add_layer_config(n_neurons=60, layer_config=ff_layer_config, var_scope="output_layer")

    #
    #
    # =====================================================================================================================
    # TRAINING CONFIGURATION
    # =====================================================================================================================
    #
    #
    lstm_train_config = LayerTrainConfig().set_config(layer_norm_enabled=False,
                                                    dropout_enabled=False,
                                                    p_dropout=.95)
    ff_train_config = LayerTrainConfig().set_config(layer_norm_enabled=False,
                                                    dropout_enabled=False,
                                                    p_dropout=.95)
    #ADAPT

    #algorithm = [AlgorithmC.LOCAL_REPARAMETRIZATION, AlgorithmC.LOCAL_REPARAMETRIZATION, AlgorithmC.REPARAMETRIZATION, AlgorithmC.REPARAMETRIZATION][int(exp_id / 24)]
    #gumbel_tau = [.3, 1., 3., .3, 1., 3., .3, 1., 3., .3, 1., 3.][int(exp_id / 8)]
    #ste = [AlgorithmC.GUMBEL_STE, AlgorithmC.NO_STE, AlgorithmC.GUMBEL_STE, AlgorithmC.NO_STE][int(exp_id / 24)]
    #gumbel_tau = np.logspace(-1, 1, 5)[task_id % 5]
    algorithm = [AlgorithmC.REPARAMETRIZATION, AlgorithmC.LOCAL_REPARAMETRIZATION][int(exp_id / 3)]
    ste_type = AlgorithmC.GUMBEL_STE
    gumbel_tau = 1.
    


    train_config = TrainConfig(task_id=task_id) \
        .set_learning_rate(learning_rate=.2)\
        .set_algorithm(algorithm=algorithm, n_forward_passes=1, ste_type=ste_type, gumbel_tau=gumbel_tau)\
        .add_layer_train_config(var_scope=["lstm_0", "lstm_1"], layer_train_config=lstm_train_config) \
        .add_layer_train_config(var_scope=["output_layer"], layer_train_config=ff_train_config) \

    #
    #
    # =====================================================================================================================
    # INFO CONFIGURATION
    # =====================================================================================================================
    #
    #
    n_grads = 100000
    info_config = InfoConfig(filename="test", timer_enabled=False, profiling_enabled=False)
    #info_config.add_model_saver(filename="toy_small", task_id=task_id)
    #info_config.add_model_saver(filenaIme="full_m_model", task_id=task_id)
    info_config.add_model_loader(filename=filename, task_id=exp_id % 3)
    info_config.add_training_metrics_saver(path="../nr_ic/t_")
    #info_config.add_gradient_saver(n_grads)

    exp = Experiment(nn_config=nn_config,
                    data_config=data_config,
                    train_config=train_config,
                    info_config=info_config,
                    task_id=task_id)

    exp.train_multiple_runs(epochs, runs)

    if write_logs_to_file:
        sys.stdout = orig_stdout
        f.close()
    # Tested: Reparametrization and local reparametrization on gaussian weights, sigmoid binary / ternary and logistic binary / ternary (no ste)

if __name__ == "__main__":
    run(5)
